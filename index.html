<!DOCTYPE html>
<html>
<head>
    <link rel="stylesheet" href="styles.css">
    <link href='https://fonts.googleapis.com/css?family=Lato:400,700' rel='stylesheet' type='text/css'>
</head>
<body>

<h1>Motivation for Project/Background</h1>
<section>
    <p>
        In my mid 20s, I turned into a bit of a mad scientist for the better part of a year!  I wanted sick music chops and had
        big problems with music education.  I was focused on real-time creativity, composition, and execution - the exciting stuff,
        not the boring and tedious stuff taught in school.  After I broke into the software industry and my chops began to develop, I started
        to think I could implement some of the wild ideas I was dreaming up, so I decided to quit my job, jump in, and see what I could produce.
          <b>My goal was ambitious: I wanted to create a software platform that could automate the acquisition of virtuosic
            musical capabilities to anyone</b>.
    </p>
    <p>
        There are various important tasks in music, e.g. recognizing notes and chords in audio recordings, generating new notes and chords on the fly,
        recognizing song structure, ..., but some musicians seemingly have better, easier, more efficient ways of completing these than others.
        Some musicians randomly choose notes within a scale without knowing what it will sound like beforehand, others already have a sound in
        their head and know which notes on the fretboard will produce those sounds. Some people use the rules of harmony to draw from a range of
        sensible chords to play after the current one, others hear a good sounding chord in their head and know the frets that produce those sounds.
        I dreamt of finding principled ways of completing and training these tasks. I also wanted to explore others, like vocal harmonizing, perfect pitch,
        the voice-instrument connection, ...
    </p>
    <p>
        What you see here are some highlights from a demo video I shot of the software prototype that I used to train myself on these various tasks.
        After a bunch of experimentation, I made progress on some abilities that seemed very out of reach before, including:

        <ul>
        <li>
            Reliable relative pitch: given a bass note sounded on my instrument, and a melody note that I imagined in my head, I could immediately find the fret corresponding to the imagine note with high accuracy
        </li>
        <li>
            Pseudo-perfect pitch: given I could sing a note, with a high accuracy I could choose the corresponding fret on the guitar +/- one semi-tone of error.
        </li>
        <li>
            Chord detection: by just hearing the raw audio straight from a bar of a song, I could sing with high accuracy the root of the chord that was sounding, and whether it was major, minor, dominant, …
        </li>
        <li>
            Given a melody, the ability to harmonize with my voice in real-time
        </li>
        <li>
            The ability to scat sing a very convincing solo over any chord progression, at least from a “note choice” perspective (my singing voice is not exactly pleasant from a timbre perspective)
        </li>
        </ul>
        .

        Given competency at enough tasks such as these, I believed you could “weave/compose” them together to achieve more complicated virtuosic tasks, such as composing
        an entire song from scratch on the fly. Ultimately, working on this project became too much effort to balance alongside of work life and social life,
        so I had to put it down and focus on other things.
</section>

<h1>How I approached the problem</h1>

<p>
    I explored the possibility that each of these various musical faculties could be trained like a machine-learning model.

    If we take that as an assumption, then maybe other things would follow:
    <ul>
    <li>
        We should prioritize supervised learning, as that is a simple and common type of learning in the industry.
    </li>
    <li>
        We should have a huge corpus of structured training data, automatically collected and labelled.
    </li>
    <li>
        We should create as tight of a feedback loop as we can.
    </li>
    <li>
        We should map a musical action to a vector (feature extraction), score it, compute the loss/cost, and feed that
        error signal back to the user.  Hopefully, if a sensible cost function is chosen, the brain has some "biological black box" method to minimize the "cost" (maximize the value/goodness)
        of a musical decision.
    </li>
    </ul>
</p>

<p>
    There was the problem of how to extract information (concise training data) from a high-dimensional digital artifact like a WAV or MP3 file.
    When we think about a song, we are interested in details like:
    <ul>
    <li>
        notes (melody, bass, ...)
    </li>
    <li>
        chords
    </li>
    <li>
        measures
    </li>
    <li>
        phrases
    </li>
    <li>
        sections
    </li>
    </ul>
    How can we take a real recording with all its complexities and turn it into its essence?
</p>

<p>
    Then, there were problems of underlying technical infrastructure:
    <ul>
        <li>
            Can we extract information (notes, chords, ...) from raw audio?
        </li>
        <li>
            Given a stream of (digital) notes, can we send it to our instrument and watch it being played, and in realtime?
        </li>
        <li>
            Can we play something on our instrument, define a musical game (i.e. a set of rules), have those rules be executed
            in realtime, and the results displayed to us in a way that is easily digestible?
        </li>
        <li>
            Can we convert our voice (a very natural way to produce notes) to a digital form, and work those into our digital, rule-processing ecosystem?
        </li>
        <li>
            How easily can we define these musical games?  (DSL for music, framework for executing that specification, ...)
        </li>
        <li>
            Can we quickly create decent, useful, and robust user interfaces?  Can they be drag-and-drop and WYSIWYG?
        </li>
    </ul>
</p>

<h1>What I created</h1>

<ul>
    <li>
        a library to convert raw audio (WAV, mp3, ...) into a digital form similar to sheet music/jazz lead sheets
    </li>
    <li>
        a framework to amass training data using the above functionality
    </li>
    <li>
        a framework to define musical games that could be played in realtime
    </li>
    <li>
        a library to stream notes to the fretboard via bluetooth
    </li>
    <li>
        a library to process MIDI notes received from guitar
    </li>
    <li>
        libraries to interface with useful applications like Ableton Live, Max, Midi/MusicXML editors, ...
    </li>
    <li>
        a few random musical analysis utilities
    </li>
</ul>
<p>
</p>

<h1>Demos/Highlights</h1>

<p>
    I ended up putting together a setup that highly simplified the playing of these musical games.  If I wanted to send information to my guitar, I could
    easily specify which fret to send it to (using a high-level programming language like Scala, TypeScript, and Python).  Moreover, I could
    specify which color I wanted the fret to take.  The colors were useful 1) to distinguish between different parts when
    multiple parts were streamed to the fretboard 2) to visualize the "error signal"/feedback during training games (e.g. colors
    "close" to green were close to the ground truth, colors "close" to red signified the most loss) and 3) when key center information was available,
    to label in real time the <b>solfege syllable</b> associated with a plucked note.

</p>
<video src="https://user-images.githubusercontent.com/6301308/202567957-3db9d9f2-660f-400c-8998-d4e54d3f0d09.mov" data-canonical-src="https://user-images.githubusercontent.com/6301308/202567957-3db9d9f2-660f-400c-8998-d4e54d3f0d09.mov" controls="controls">
</video>

<p>
    Here is an example of multiple parts being played on the fretboard, and one part being labelled with the corresponding
    solfege colors as I play it.
</p>
<video src="https://user-images.githubusercontent.com/6301308/202567991-a5eff804-f163-4f96-8644-1036c81fe85b.mov" data-canonical-src="https://user-images.githubusercontent.com/6301308/202567991-a5eff804-f163-4f96-8644-1036c81fe85b.mov" controls="controls">
</video>

<p>
    Choosing notes to play on an instrument is really unintuitive if you think about it.  Most people can easily and naturally mimic a
    melody they've heard, but how many of those same people can play it just as fast on an instrument, even if they're professional?
    Almost none.
</p>
<p>
    In response to this idea, I wrote software to map my voice to its corresponding fret in realtime, in order to make finding
    a note on the guitar as natural as finding it with my voice.
</p>
<video src="https://user-images.githubusercontent.com/6301308/202568016-d87efb6e-6005-423f-94a5-2c043c591938.mov" data-canonical-src="https://user-images.githubusercontent.com/6301308/202568016-d87efb6e-6005-423f-94a5-2c043c591938.mov" controls="controls">
</video>

<p>
    Before I had thought of using an LED mesh on a real guitar, my idea was to use a touch surface and program any musical
    interface I wanted (here is one that roughly simulates a guitar).  Though the possibilities were endless with respect
    to designing your instrument, it was too annoying to play everything on a flat, rectangular surface.  It wasn't comfortable
    nor satisfying.  I was glad to discover a better way to achieve this...
</p>
<video src="https://user-images.githubusercontent.com/6301308/202568030-a4ef6172-b2de-4f93-be8d-597b3d2817a4.mov" data-canonical-src="https://user-images.githubusercontent.com/6301308/202568030-a4ef6172-b2de-4f93-be8d-597b3d2817a4.mov" controls="controls">
</video>

<video src="https://user-images.githubusercontent.com/6301308/202568047-259ff329-21aa-4fdb-87b1-5171c3f3dd4a.mov" data-canonical-src="https://user-images.githubusercontent.com/6301308/202568047-259ff329-21aa-4fdb-87b1-5171c3f3dd4a.mov" controls="controls">
</video>

<p>
    Humans listen to songs on YouTube and Spotify, not books of sheet music, so I wanted to be able to extract training material
    from those sources automatically and quickly - here is an example of doing so from YouTube.
</p>
<video src="https://user-images.githubusercontent.com/6301308/202568093-32c1e432-da24-463d-ba65-f95b4074e056.mov" data-canonical-src="https://user-images.githubusercontent.com/6301308/202568093-32c1e432-da24-463d-ba65-f95b4074e056.mov" controls="controls">
</video>

<p>
    Here's an example of interfacing with other applications - we can also see that the software representation of the automatically transcribed song could be rendered to sheet music,
    producing a jazz "lead sheet"-like artifact.
</p>
<video src="https://user-images.githubusercontent.com/6301308/202568190-f66bd058-a586-4c95-b5ad-88458af53a7a.mov" data-canonical-src="https://user-images.githubusercontent.com/6301308/202568190-f66bd058-a586-4c95-b5ad-88458af53a7a.mov" controls="controls">
</video>

<p>
    I love working backwards when solving problems in general, so I wanted to do the same thing when playing these musical games over songs.
    Slowing things down is also great, in order to give your brain time to process all this information.  As for speeding things up, I was under the impression
    that by practicing at higher speeds, playing at a regular tempo would be a piece of cake.
</p>
<video src="https://user-images.githubusercontent.com/6301308/202568068-f816ffd0-ef98-4ab4-8710-04bc711f32b8.mov" data-canonical-src="https://user-images.githubusercontent.com/6301308/202568068-f816ffd0-ef98-4ab4-8710-04bc711f32b8.mov" controls="controls">
</video>

<p>
    Key centers are important, but they can change frequently, especially in jazz, so I wanted a robust method to estimate them with software.
</p>
<video src="https://user-images.githubusercontent.com/6301308/202568104-65aa5675-169c-4f41-9e90-bd75bd0f77dc.mov" data-canonical-src="https://user-images.githubusercontent.com/6301308/202568104-65aa5675-169c-4f41-9e90-bd75bd0f77dc.mov" controls="controls">
</video>

<p>
    I didn't want to loop over measures, I wanted to loop over meaningful things like phrases, verses, choruses, ... so I used a model
    that could help me achieve that.
</p>
<video src="https://user-images.githubusercontent.com/6301308/202568125-8e916b7c-6394-4d31-8d7a-6fa249467f5e.mov" data-canonical-src="https://user-images.githubusercontent.com/6301308/202568125-8e916b7c-6394-4d31-8d7a-6fa249467f5e.mov" controls="controls">
</video>

<p>
    I was really a perfectionist about rhythm - I wanted to be able to loop over a target phrase indefinitely, and get into
    a trance-like state, so I didn't want any abrupt starting and stopping, pausing, gaps, ...
</p>
<video src="https://user-images.githubusercontent.com/6301308/202568157-f0309ce6-b5a2-45bd-8de5-3081d7f71773.mov" data-canonical-src="https://user-images.githubusercontent.com/6301308/202568157-f0309ce6-b5a2-45bd-8de5-3081d7f71773.mov" controls="controls">
</video>

<p>
    Of course, not all recordings in the wild keep the same tempo throughout, but I still wanted to automatically produce a "sheet music like"
    representation of them.
</p>
<video src="https://user-images.githubusercontent.com/6301308/202568141-d26db088-8013-4332-9b59-0cf8a8bf0551.mov" data-canonical-src="https://user-images.githubusercontent.com/6301308/202568141-d26db088-8013-4332-9b59-0cf8a8bf0551.mov" controls="controls">
</video>

<p>
    This one didn't end up being too useful, but it was kind of cool... I converted a vocal signal into a Hertz timeseries
    and fed it into a synthesizer
</p>
<video src="https://user-images.githubusercontent.com/6301308/202568174-a3233659-aa18-4edf-b108-94e0006e54fa.mov" data-canonical-src="https://user-images.githubusercontent.com/6301308/202568174-a3233659-aa18-4edf-b108-94e0006e54fa.mov" controls="controls">
</video>

<p>
    Here's a few examples of the musical "games" I created and played.  Of course there are a lot of things in common
    with all these games, so I created a software framework that took care of all that common functionality, along with places
    I could inject my own game-specific rules.
</p>
<video src="https://user-images.githubusercontent.com/6301308/202568205-e56e5f31-45ba-4477-9a7f-922de86a42dd.mov" data-canonical-src="https://user-images.githubusercontent.com/6301308/202568205-e56e5f31-45ba-4477-9a7f-922de86a42dd.mov" controls="controls">
</video>

<video src="https://user-images.githubusercontent.com/6301308/202568225-3bfc8742-3cd7-42c5-9bb7-75b27a8e9acb.mov" data-canonical-src="https://user-images.githubusercontent.com/6301308/202568225-3bfc8742-3cd7-42c5-9bb7-75b27a8e9acb.mov" controls="controls">
</video>

<video src="https://user-images.githubusercontent.com/6301308/202568245-4347309f-3a62-45be-aa35-d023fe6b3d54.mov" data-canonical-src="https://user-images.githubusercontent.com/6301308/202568245-4347309f-3a62-45be-aa35-d023fe6b3d54.mov" controls="controls">
</video>

<video src="https://user-images.githubusercontent.com/6301308/202568256-fff6f3fc-ed73-44ae-a8de-70415db1eb37.mov" data-canonical-src="https://user-images.githubusercontent.com/6301308/202568256-fff6f3fc-ed73-44ae-a8de-70415db1eb37.mov" controls="controls">
</video>

<video src="https://user-images.githubusercontent.com/6301308/202568270-67fd74e8-6f96-45ee-92c3-913771fd1fa5.mov" data-canonical-src="https://user-images.githubusercontent.com/6301308/202568270-67fd74e8-6f96-45ee-92c3-913771fd1fa5.mov" controls="controls">
</video>

<p>
    You could play this guitar just like any other electric guitar - of course, playing something like this
    would be impossible on a touch surface like an iPad.
</p>

<video src="https://user-images.githubusercontent.com/6301308/202567942-3a1086b3-eea1-48c6-a88f-7b19d3b8c279.mov" data-canonical-src="https://user-images.githubusercontent.com/6301308/202567942-3a1086b3-eea1-48c6-a88f-7b19d3b8c279.mov" controls="controls">
</video>

<p>
    Since I primarily drove the software with Ableton Live (and Max), I decided to learn how to produce music inside of the DAW as well.
    I did a deep dive for a couple weeks, and made this proof-of-concept EDM remix to show that you could produce real music
    alongside these software libraries.
</p>

<iframe src="https://www.youtube.com/embed/0fUr-IYLA0U"></iframe>

</body>
</html>
